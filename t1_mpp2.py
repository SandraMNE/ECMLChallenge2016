######################################################################
#    ECML/PKDD 2016 Discovery Challenge (Bank data)                	 #
#    Task 1 implementation - model construction part (parallelized)  #
#                                                                    #
#    Implemented by Sandra Mitrovic & Gaurav Singh Chauhan           #
######################################################################

import pandas as pd
import numpy as np

from sklearn import preprocessing, cross_validation, metrics, linear_model, cluster
from sklearn.preprocessing import normalize, OneHotEncoder
# from sklearn.cross_validation import StratifiedKFold
# from sklearn.linear_model import LinearRegression
# from sklearn.decomposition import PCA

# from sklearn.neighbors import KNeighborsClassifier
# from sklearn.cross_validation import train_test_split
from sklearn.ensemble import RandomForestRegressor as RFR
from sklearn.ensemble import GradientBoostingRegressor 
from sklearn.ensemble import AdaBoostRegressor as ABR
# from sklearn.metrics import roc_auc_score as AUC


from multiprocessing import Pool
from sklearn.preprocessing import MinMaxScaler 

# import datetime
# import math
# import random
import json

from lasagne.layers import DenseLayer
from lasagne.layers import DropoutLayer
from lasagne.layers import InputLayer
from lasagne.nonlinearities import softmax
from nolearn.lasagne import NeuralNet
from lasagne.nonlinearities import sigmoid, tanh, rectify

import sys
from data_branch import BranchData
from evaluator_branch import BranchEvaluator
from sklearn.preprocessing import StandardScaler

import locale
# locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')
locale.setlocale(locale.LC_ALL, 'usa')

# number of parallel processes
n_proc = 30

#### loading data from files generated by t1_mpp1.py ###
X_train = np.array(json.loads(open('X_train_t1.txt','r').read()))
X_test = np.array(json.loads(open('X_test_t1.txt','r').read()),dtype=np.float32)
Y_train = np.array(json.loads(open('Y_train_t1.txt','r').read()))
rev_branch_dict = json.loads(open('rev_branch_dict_t1.txt','r').read())	# branch dictionary
userids_test = json.loads(open('userids_test_t1.txt','r').read()) 

rev_branch_dict = {int(k):int(v) for k,v in rev_branch_dict.items()} # just a conversion

dist_train = np.array(json.loads(open('dist_train_t1.txt','r').read()))  # user-branch distance for train data 
dist_test = np.array(json.loads(open('dist_test_t1.txt','r').read()))

act_branch_dist_train = np.array(json.loads(open('act_branch_dist_train.txt','r').read())) # mean user activity-branch distance for train data
act_branch_dist_test = np.array(json.loads(open('act_branch_dist_test.txt','r').read()))
#### end of loading data ####

# per each user, we retain only number of visits for 5 top visited branches 
for i in range(Y_train.shape[0]):
	selected = np.argsort(Y_train[i,:])[::-1]
	Y_train[i,selected[5:]] = 0

# and we normalize it
Y_train = normalize(Y_train, norm='l2', axis=1, copy=False)

# we tried also to apply scaling
# mmscaler = MinMaxScaler(feature_range=(-1, 1))
# mmscaler.fit(np.vstack((X_train,X_test)))
# X_train = mmscaler.transform(X_train)
# X_test = mmscaler.transform(X_test)


# NN - train
def regr(X, Y):
	l = InputLayer(shape=(None, X.shape[1]))
	l = DenseLayer(l, num_units=Y.shape[1]+100, nonlinearity=tanh)
	# l = DropoutLayer(l, p=0.3, rescale=True)  # previous: p=0.5
	l = DenseLayer(l, num_units=Y.shape[1]+50, nonlinearity=tanh)
	# l = DropoutLayer(l, p=0.3, rescale=True)  # previous: p=0.5
	l = DenseLayer(l, num_units=Y.shape[1], nonlinearity=None)
	net = NeuralNet(l, regression=True, update_learning_rate=0.1, verbose=1)
	net.fit(X, Y)
	print(net.score(X, Y))
	return net

# NN - predict
def predict(net, X_test):
	return net.predict(X_test)


# function for training and predicting i-th Regressor (per i-th branch)  
def train_and_score(i):
	global X_train
	global X_test 
	global Y_train
	global dist_train
	global dist_test
	
	# GBR performed best but we experimented with other models as well (see the paper)
	cl = GradientBoostingRegressor(n_estimators=100, loss='ls', learning_rate=0.1)

	# we add user distance from i-th branch (for which we do prediction) to train set
	dist_from_target_branch_train = dist_train[:,i].reshape((len(dist_train[:,i]),1))  # dist from i-th branch
	X_train = np.hstack((X_train, dist_from_target_branch_train))
	# we add mean user activity distance from i-th branch (for which we do prediction) to train set
	ab_dist_train = act_branch_dist_train[:,i].reshape((len(act_branch_dist_train[:,i]),1))  # dist from i-th branch
	X_train = np.hstack((X_train, ab_dist_train))

	# we also experimented with Standard Scaler, without much success
	# mmscaler_train = StandardScaler()
	# X_train = mmscaler_train.fit_transform(X_train)

	cl.fit(X_train,Y_train[:,i])

	# same as above for test set
	dist_from_target_branch_test = dist_test[:,i].reshape((len(dist_test[:,i]),1))  # dist from i-th branch
	X_test = np.hstack((X_test, dist_from_target_branch_test))
	ab_dist_test = act_branch_dist_test[:,i].reshape((len(act_branch_dist_test[:,i]),1))  # dist from i-th branch
	X_test = np.hstack((X_test, ab_dist_test))

	# mmscaler_test = StandardScaler()
	# X_test = mmscaler_test.fit_transform(X_test)

	return cl.predict(X_test)


# main program
if __name__=='__main__':

	# prediction result is initialized as empty array
	pred = []

	# pool of 323 processes is created; each process will invoke train_and_score function to train separate model and perform prediction per 1 branch
	pool = Pool(processes=n_proc)
	params = [i for i in range(323)]
	# params = [Y_train[:,i] for i in range(323)]
	it = pool.map(train_and_score, params)

	for res in it:
		pred.append(res.tolist())
	pool.terminate()

	pred = np.array(pred).T
   
	# PCA attempt: not working
	# dim_red = PCA(n_components=200) #, copy=True, whiten=False)[source]
	# X_train = dim_red.fit_transform(X_train)
	# X_test = dim_red.fit_transform(X_test)

	# code for NN: first we scale then train the network and predict
	# mmscaler = MinMaxScaler(feature_range=(-1, 1))
	# mmscaler.fit(np.vstack((X_train,X_test)))
	# X_train = mmscaler.transform(X_train)
	# X_test = mmscaler.transform(X_test)

	# net = regr(X_train, Y_train)
	# pred = predict(net, X_test)


	# writing results
	res = pd.DataFrame(columns = ['#USER_ID','POI_ID','NUMBER_OF_VISITS'])
	u = [] # to store user id
	b = [] # to store branch id
	num = [] # to store number of visits

	# we select five most visited branches
	selected = np.argsort(pred,axis=1)[:,pred.shape[1]-5::]

	# if prediction is not 0, we add test user id & branch id to appropriate arrays
	for i in range(selected.shape[0]):
		for j in selected[i]:
			if pred[i,j]!=0:
				u.append(userids_test[i])
				b.append(rev_branch_dict[j])
				num.append(pred[i,j])


	# storing arrays to data frame 
	res['#USER_ID'] = list(map(int,u))
	res['POI_ID'] = b
	res['NUMBER_OF_VISITS'] = num 
	
	# writing data frame to submission file
	res.to_csv('pred.csv',delimiter=',',index=False)
